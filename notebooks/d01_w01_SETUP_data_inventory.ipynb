{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4f4de99",
   "metadata": {},
   "source": [
    "## 1. Setup & Environment Configuration\n",
    "\n",
    "**Objective:** Import required libraries, configure paths, validate environment\n",
    "\n",
    "**Activities:**\n",
    "- Import pandas, numpy, dask for data manipulation\n",
    "- Define path constants for data/raw/ and docs/\n",
    "- Test imports and display versions\n",
    "- Configure warnings and display settings\n",
    "\n",
    "**Expected output:** Confirmation that environment is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d15cf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package Versions:\n",
      "  pandas: 2.1.4\n",
      "  numpy: 1.26.4\n",
      "  dask: 2025.11.0\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Package versions\n",
    "print(\"Package Versions:\")\n",
    "print(f\"  pandas: {pd.__version__}\")\n",
    "print(f\"  numpy: {np.__version__}\")\n",
    "print(f\"  dask: {dask.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34e365ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK - Display settings configured\n"
     ]
    }
   ],
   "source": [
    "# Configure environment\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"OK - Display settings configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f19275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\adiaz\\OneDrive\\Dokumente\\PythonScripts\\MasterClass\\Demand-forecasting-in-retail\n",
      "OK - Paths validated:\n",
      "  DATA_RAW: C:\\Users\\adiaz\\OneDrive\\Dokumente\\PythonScripts\\MasterClass\\Demand-forecasting-in-retail\\data\\raw\n",
      "  DOCS: C:\\Users\\adiaz\\OneDrive\\Dokumente\\PythonScripts\\MasterClass\\Demand-forecasting-in-retail\\docs\n"
     ]
    }
   ],
   "source": [
    "# Determine current directory (works in both scripts and notebooks)\n",
    "current_dir = Path(__file__).parent if '__file__' in globals() else Path.cwd()\n",
    "project_root = current_dir.parent if current_dir.name == 'notebooks' else current_dir\n",
    "print(f\"Project root: {project_root.resolve()}\")\n",
    "\n",
    "# Define path constants relative to project root\n",
    "DATA_RAW = project_root / 'data' / 'raw'\n",
    "DOCS = project_root / 'docs'\n",
    "\n",
    "# Verify paths exist\n",
    "assert DATA_RAW.exists(), f\"ERROR - Path not found: {DATA_RAW}\"\n",
    "assert DOCS.exists(), f\"ERROR - Path not found: {DOCS}\"\n",
    "\n",
    "print(\"OK - Paths validated:\")\n",
    "print(f\"  DATA_RAW: {DATA_RAW.resolve()}\")\n",
    "print(f\"  DOCS: {DOCS.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70d7ef9",
   "metadata": {},
   "source": [
    "## 2. Load Support Files (Small CSVs)\n",
    "\n",
    "**Objective:** Load and validate all small CSV files (stores, items, oil, holidays, transactions)\n",
    "\n",
    "**Activities:**\n",
    "- Load 5 support CSV files into pandas DataFrames\n",
    "- Display shape, columns, and data types for each\n",
    "- Check for missing values\n",
    "- Convert date columns to datetime format\n",
    "- Display first few rows for validation\n",
    "\n",
    "**Expected output:** \n",
    "- 5 DataFrames loaded successfully\n",
    "- Schema validation report\n",
    "- Missing value counts per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e46b518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('c:/Users/adiaz/OneDrive/Dokumente/PythonScripts/MasterClass/Demand-forecasting-in-retail/data/raw/stores.csv')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_RAW / 'stores.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "702dff4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stores.csv loaded:\n",
      "  Shape: (54, 5)\n",
      "  Columns: ['store_nbr', 'city', 'state', 'type', 'cluster']\n",
      "  Missing values: 0\n",
      "\n",
      "First 3 rows:\n",
      "   store_nbr   city      state type  cluster\n",
      "0          1  Quito  Pichincha    D       13\n",
      "1          2  Quito  Pichincha    D       13\n",
      "2          3  Quito  Pichincha    D        8\n"
     ]
    }
   ],
   "source": [
    "# Load stores.csv\n",
    "df_stores = pd.read_csv(DATA_RAW / 'stores.csv')\n",
    "\n",
    "print(\"stores.csv loaded:\")\n",
    "print(f\"  Shape: {df_stores.shape}\")\n",
    "print(f\"  Columns: {list(df_stores.columns)}\")\n",
    "print(f\"  Missing values: {df_stores.isnull().sum().sum()}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_stores.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14da7ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items.csv loaded:\n",
      "  Shape: (4100, 4)\n",
      "  Columns: ['item_nbr', 'family', 'class', 'perishable']\n",
      "  Missing values: 0\n",
      "\n",
      "First 3 rows:\n",
      "   item_nbr     family  class  perishable\n",
      "0     96995  GROCERY I   1093           0\n",
      "1     99197  GROCERY I   1067           0\n",
      "2    103501   CLEANING   3008           0\n"
     ]
    }
   ],
   "source": [
    "# Load items.csv\n",
    "df_items = pd.read_csv(DATA_RAW / 'items.csv')\n",
    "\n",
    "print(\"items.csv loaded:\")\n",
    "print(f\"  Shape: {df_items.shape}\")\n",
    "print(f\"  Columns: {list(df_items.columns)}\")\n",
    "print(f\"  Missing values: {df_items.isnull().sum().sum()}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_items.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db037bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oil.csv loaded:\n",
      "  Shape: (1218, 2)\n",
      "  Columns: ['date', 'dcoilwtico']\n",
      "  Missing values: 43\n",
      "\n",
      "Data types:\n",
      "date           object\n",
      "dcoilwtico    float64\n",
      "dtype: object\n",
      "\n",
      "First 3 rows:\n",
      "         date  dcoilwtico\n",
      "0  2013-01-01         NaN\n",
      "1  2013-01-02       93.14\n",
      "2  2013-01-03       92.97\n"
     ]
    }
   ],
   "source": [
    "# Load oil.csv\n",
    "df_oil = pd.read_csv(DATA_RAW / 'oil.csv')\n",
    "\n",
    "print(\"oil.csv loaded:\")\n",
    "print(f\"  Shape: {df_oil.shape}\")\n",
    "print(f\"  Columns: {list(df_oil.columns)}\")\n",
    "print(f\"  Missing values: {df_oil.isnull().sum().sum()}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df_oil.dtypes)\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_oil.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcdbce8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holidays_events.csv loaded:\n",
      "  Shape: (350, 6)\n",
      "  Columns: ['date', 'type', 'locale', 'locale_name', 'description', 'transferred']\n",
      "  Missing values: 0\n",
      "\n",
      "Data types:\n",
      "date           object\n",
      "type           object\n",
      "locale         object\n",
      "locale_name    object\n",
      "description    object\n",
      "transferred      bool\n",
      "dtype: object\n",
      "\n",
      "First 3 rows:\n",
      "         date     type    locale locale_name                    description  \\\n",
      "0  2012-03-02  Holiday     Local       Manta             Fundacion de Manta   \n",
      "1  2012-04-01  Holiday  Regional    Cotopaxi  Provincializacion de Cotopaxi   \n",
      "2  2012-04-12  Holiday     Local      Cuenca            Fundacion de Cuenca   \n",
      "\n",
      "   transferred  \n",
      "0        False  \n",
      "1        False  \n",
      "2        False  \n"
     ]
    }
   ],
   "source": [
    "# Load holidays_events.csv\n",
    "df_holidays = pd.read_csv(DATA_RAW / 'holidays_events.csv')\n",
    "\n",
    "print(\"holidays_events.csv loaded:\")\n",
    "print(f\"  Shape: {df_holidays.shape}\")\n",
    "print(f\"  Columns: {list(df_holidays.columns)}\")\n",
    "print(f\"  Missing values: {df_holidays.isnull().sum().sum()}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df_holidays.dtypes)\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_holidays.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc5ebd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transactions.csv loaded:\n",
      "  Shape: (83488, 3)\n",
      "  Columns: ['date', 'store_nbr', 'transactions']\n",
      "  Missing values: 0\n",
      "\n",
      "Data types:\n",
      "date            object\n",
      "store_nbr        int64\n",
      "transactions     int64\n",
      "dtype: object\n",
      "\n",
      "First 3 rows:\n",
      "         date  store_nbr  transactions\n",
      "0  2013-01-01         25           770\n",
      "1  2013-01-02          1          2111\n",
      "2  2013-01-02          2          2358\n"
     ]
    }
   ],
   "source": [
    "# Load transactions.csv\n",
    "df_transactions = pd.read_csv(DATA_RAW / 'transactions.csv')\n",
    "\n",
    "print(\"transactions.csv loaded:\")\n",
    "print(f\"  Shape: {df_transactions.shape}\")\n",
    "print(f\"  Columns: {list(df_transactions.columns)}\")\n",
    "print(f\"  Missing values: {df_transactions.isnull().sum().sum()}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df_transactions.dtypes)\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_transactions.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed0a7016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Files Summary:\n",
      "               File  Rows  Columns  Missing\n",
      "         stores.csv    54        5        0\n",
      "          items.csv  4100        4        0\n",
      "            oil.csv  1218        2       43\n",
      "holidays_events.csv   350        6        0\n",
      "   transactions.csv 83488        3        0\n",
      "\n",
      "OK - All 5 support files loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Create summary dictionary\n",
    "support_files_summary = {\n",
    "    'File': ['stores.csv', 'items.csv', 'oil.csv', 'holidays_events.csv', 'transactions.csv'],\n",
    "    'Rows': [len(df_stores), len(df_items), len(df_oil), len(df_holidays), len(df_transactions)],\n",
    "    'Columns': [df_stores.shape[1], df_items.shape[1], df_oil.shape[1], df_holidays.shape[1], df_transactions.shape[1]],\n",
    "    'Missing': [0, 0, 43, 0, 0]\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(support_files_summary)\n",
    "print(\"Support Files Summary:\")\n",
    "print(df_summary.to_string(index=False))\n",
    "print(\"\\nOK - All 5 support files loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bb81d4",
   "metadata": {},
   "source": [
    "## 3. Inspect train.csv with Dask\n",
    "\n",
    "**Objective:** Load large train.csv file using Dask and inspect structure without loading full dataset into memory\n",
    "\n",
    "**Activities:**\n",
    "- Use Dask to read train.csv (479 MB file)\n",
    "- Display schema and estimated row count\n",
    "- Check for missing values\n",
    "- Sample first rows for validation\n",
    "- Document file characteristics\n",
    "\n",
    "**Expected output:** \n",
    "- Train data structure confirmed\n",
    "- Missing value percentages calculated\n",
    "- Memory-efficient inspection complete\n",
    "\n",
    "**Note:** train.csv is too large for pandas (125M rows). Dask enables lazy evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc743af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train.csv with Dask (this may take a moment)...\n",
      "OK - train.csv loaded (Dask DataFrame)\n",
      "\n",
      "Columns: ['id', 'date', 'store_nbr', 'item_nbr', 'unit_sales', 'onpromotion']\n",
      "Data types:\n",
      "id                       int64\n",
      "date           string[pyarrow]\n",
      "store_nbr                int64\n",
      "item_nbr                 int64\n",
      "unit_sales             float64\n",
      "onpromotion            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load train.csv with Dask (lazy evaluation)\n",
    "print(\"Loading train.csv with Dask (this may take a moment)...\")\n",
    "df_train = dd.read_csv(DATA_RAW / 'train.csv')\n",
    "\n",
    "print(\"OK - train.csv loaded (Dask DataFrame)\")\n",
    "print(f\"\\nColumns: {list(df_train.columns)}\")\n",
    "print(f\"Data types:\")\n",
    "print(df_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4dee56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing row count (this will take time - processing 125M rows)...\n",
      "OK - Total rows: 125,497,040\n",
      "\n",
      "Computing missing values per column...\n",
      "\n",
      "Missing Values:\n",
      "  id                         0 (   0.0%)\n",
      "  date                       0 (   0.0%)\n",
      "  store_nbr                  0 (   0.0%)\n",
      "  item_nbr                   0 (   0.0%)\n",
      "  unit_sales                 0 (   0.0%)\n",
      "  onpromotion       21,657,651 ( 17.26%)\n"
     ]
    }
   ],
   "source": [
    "# Compute actual row count (triggers computation)\n",
    "print(\"Computing row count (this will take time - processing 125M rows)...\")\n",
    "train_length = len(df_train)\n",
    "print(f\"OK - Total rows: {train_length:,}\")\n",
    "\n",
    "# Check missing values per column\n",
    "print(\"\\nComputing missing values per column...\")\n",
    "missing_counts = df_train.isnull().sum().compute()\n",
    "missing_pct = (missing_counts / train_length * 100).round(2)\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "for col in df_train.columns:\n",
    "    count = missing_counts[col]\n",
    "    pct = missing_pct[col]\n",
    "    print(f\"  {col:<15} {count:>12,} ({pct:>6}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b61e518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling first 1000 rows...\n"
     ]
    }
   ],
   "source": [
    "# Sample first 1000 rows to inspect data\n",
    "print(\"Sampling first 1000 rows...\")\n",
    "df_train_sample = df_train.head(1000, npartitions=-1)\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_train_sample.head())\n",
    "\n",
    "print(\"\\nBasic statistics for unit_sales:\")\n",
    "print(df_train_sample['unit_sales'].describe())\n",
    "\n",
    "# Check for negative values\n",
    "negative_count = (df_train_sample['unit_sales'] < 0).sum()\n",
    "print(f\"\\nNegative unit_sales in sample: {negative_count} rows\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
