{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4f4de99",
   "metadata": {},
   "source": [
    "## 1. Setup & Environment Configuration\n",
    "\n",
    "**Objective:** Import required libraries, configure paths, validate environment\n",
    "\n",
    "**Activities:**\n",
    "- Import pandas, numpy, dask for data manipulation\n",
    "- Define path constants for data/raw/ and docs/\n",
    "- Test imports and display versions\n",
    "- Configure warnings and display settings\n",
    "\n",
    "**Expected output:** Confirmation that environment is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d15cf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package Versions:\n",
      "  pandas: 2.1.4\n",
      "  numpy: 1.26.4\n",
      "  dask: 2025.11.0\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Package versions\n",
    "print(\"Package Versions:\")\n",
    "print(f\"  pandas: {pd.__version__}\")\n",
    "print(f\"  numpy: {np.__version__}\")\n",
    "print(f\"  dask: {dask.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34e365ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK - Display settings configured\n"
     ]
    }
   ],
   "source": [
    "# Configure environment\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"OK - Display settings configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f19275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\adiaz\\OneDrive\\Dokumente\\PythonScripts\\MasterClass\\Demand-forecasting-in-retail\n",
      "OK - Paths validated:\n",
      "  DATA_RAW: C:\\Users\\adiaz\\OneDrive\\Dokumente\\PythonScripts\\MasterClass\\Demand-forecasting-in-retail\\data\\raw\n",
      "  DOCS: C:\\Users\\adiaz\\OneDrive\\Dokumente\\PythonScripts\\MasterClass\\Demand-forecasting-in-retail\\docs\n"
     ]
    }
   ],
   "source": [
    "# Determine current directory (works in both scripts and notebooks)\n",
    "current_dir = Path(__file__).parent if '__file__' in globals() else Path.cwd()\n",
    "project_root = current_dir.parent if current_dir.name == 'notebooks' else current_dir\n",
    "print(f\"Project root: {project_root.resolve()}\")\n",
    "\n",
    "# Define path constants relative to project root\n",
    "DATA_RAW = project_root / 'data' / 'raw'\n",
    "DOCS = project_root / 'docs'\n",
    "\n",
    "# Verify paths exist\n",
    "assert DATA_RAW.exists(), f\"ERROR - Path not found: {DATA_RAW}\"\n",
    "assert DOCS.exists(), f\"ERROR - Path not found: {DOCS}\"\n",
    "\n",
    "print(\"OK - Paths validated:\")\n",
    "print(f\"  DATA_RAW: {DATA_RAW.resolve()}\")\n",
    "print(f\"  DOCS: {DOCS.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70d7ef9",
   "metadata": {},
   "source": [
    "## 2. Load Support Files (Small CSVs)\n",
    "\n",
    "**Objective:** Load and validate all small CSV files (stores, items, oil, holidays, transactions)\n",
    "\n",
    "**Activities:**\n",
    "- Load 5 support CSV files into pandas DataFrames\n",
    "- Display shape, columns, and data types for each\n",
    "- Check for missing values\n",
    "- Convert date columns to datetime format\n",
    "- Display first few rows for validation\n",
    "\n",
    "**Expected output:** \n",
    "- 5 DataFrames loaded successfully\n",
    "- Schema validation report\n",
    "- Missing value counts per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e46b518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('c:/Users/adiaz/OneDrive/Dokumente/PythonScripts/MasterClass/Demand-forecasting-in-retail/data/raw/stores.csv')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_RAW / 'stores.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "702dff4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stores.csv loaded:\n",
      "  Shape: (54, 5)\n",
      "  Columns: ['store_nbr', 'city', 'state', 'type', 'cluster']\n",
      "  Missing values: 0\n",
      "\n",
      "First 3 rows:\n",
      "   store_nbr   city      state type  cluster\n",
      "0          1  Quito  Pichincha    D       13\n",
      "1          2  Quito  Pichincha    D       13\n",
      "2          3  Quito  Pichincha    D        8\n"
     ]
    }
   ],
   "source": [
    "# Load stores.csv\n",
    "df_stores = pd.read_csv(DATA_RAW / 'stores.csv')\n",
    "\n",
    "print(\"stores.csv loaded:\")\n",
    "print(f\"  Shape: {df_stores.shape}\")\n",
    "print(f\"  Columns: {list(df_stores.columns)}\")\n",
    "print(f\"  Missing values: {df_stores.isnull().sum().sum()}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_stores.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14da7ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items.csv loaded:\n",
      "  Shape: (4100, 4)\n",
      "  Columns: ['item_nbr', 'family', 'class', 'perishable']\n",
      "  Missing values: 0\n",
      "\n",
      "First 3 rows:\n",
      "   item_nbr     family  class  perishable\n",
      "0     96995  GROCERY I   1093           0\n",
      "1     99197  GROCERY I   1067           0\n",
      "2    103501   CLEANING   3008           0\n"
     ]
    }
   ],
   "source": [
    "# Load items.csv\n",
    "df_items = pd.read_csv(DATA_RAW / 'items.csv')\n",
    "\n",
    "print(\"items.csv loaded:\")\n",
    "print(f\"  Shape: {df_items.shape}\")\n",
    "print(f\"  Columns: {list(df_items.columns)}\")\n",
    "print(f\"  Missing values: {df_items.isnull().sum().sum()}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_items.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db037bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oil.csv loaded:\n",
      "  Shape: (1218, 2)\n",
      "  Columns: ['date', 'dcoilwtico']\n",
      "  Missing values: 43\n",
      "\n",
      "Data types:\n",
      "date           object\n",
      "dcoilwtico    float64\n",
      "dtype: object\n",
      "\n",
      "First 3 rows:\n",
      "         date  dcoilwtico\n",
      "0  2013-01-01         NaN\n",
      "1  2013-01-02       93.14\n",
      "2  2013-01-03       92.97\n"
     ]
    }
   ],
   "source": [
    "# Load oil.csv\n",
    "df_oil = pd.read_csv(DATA_RAW / 'oil.csv')\n",
    "\n",
    "print(\"oil.csv loaded:\")\n",
    "print(f\"  Shape: {df_oil.shape}\")\n",
    "print(f\"  Columns: {list(df_oil.columns)}\")\n",
    "print(f\"  Missing values: {df_oil.isnull().sum().sum()}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df_oil.dtypes)\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_oil.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcdbce8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holidays_events.csv loaded:\n",
      "  Shape: (350, 6)\n",
      "  Columns: ['date', 'type', 'locale', 'locale_name', 'description', 'transferred']\n",
      "  Missing values: 0\n",
      "\n",
      "Data types:\n",
      "date           object\n",
      "type           object\n",
      "locale         object\n",
      "locale_name    object\n",
      "description    object\n",
      "transferred      bool\n",
      "dtype: object\n",
      "\n",
      "First 3 rows:\n",
      "         date     type    locale locale_name                    description  \\\n",
      "0  2012-03-02  Holiday     Local       Manta             Fundacion de Manta   \n",
      "1  2012-04-01  Holiday  Regional    Cotopaxi  Provincializacion de Cotopaxi   \n",
      "2  2012-04-12  Holiday     Local      Cuenca            Fundacion de Cuenca   \n",
      "\n",
      "   transferred  \n",
      "0        False  \n",
      "1        False  \n",
      "2        False  \n"
     ]
    }
   ],
   "source": [
    "# Load holidays_events.csv\n",
    "df_holidays = pd.read_csv(DATA_RAW / 'holidays_events.csv')\n",
    "\n",
    "print(\"holidays_events.csv loaded:\")\n",
    "print(f\"  Shape: {df_holidays.shape}\")\n",
    "print(f\"  Columns: {list(df_holidays.columns)}\")\n",
    "print(f\"  Missing values: {df_holidays.isnull().sum().sum()}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df_holidays.dtypes)\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_holidays.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc5ebd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transactions.csv loaded:\n",
      "  Shape: (83488, 3)\n",
      "  Columns: ['date', 'store_nbr', 'transactions']\n",
      "  Missing values: 0\n",
      "\n",
      "Data types:\n",
      "date            object\n",
      "store_nbr        int64\n",
      "transactions     int64\n",
      "dtype: object\n",
      "\n",
      "First 3 rows:\n",
      "         date  store_nbr  transactions\n",
      "0  2013-01-01         25           770\n",
      "1  2013-01-02          1          2111\n",
      "2  2013-01-02          2          2358\n"
     ]
    }
   ],
   "source": [
    "# Load transactions.csv\n",
    "df_transactions = pd.read_csv(DATA_RAW / 'transactions.csv')\n",
    "\n",
    "print(\"transactions.csv loaded:\")\n",
    "print(f\"  Shape: {df_transactions.shape}\")\n",
    "print(f\"  Columns: {list(df_transactions.columns)}\")\n",
    "print(f\"  Missing values: {df_transactions.isnull().sum().sum()}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df_transactions.dtypes)\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_transactions.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed0a7016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Files Summary:\n",
      "               File  Rows  Columns  Missing\n",
      "         stores.csv    54        5        0\n",
      "          items.csv  4100        4        0\n",
      "            oil.csv  1218        2       43\n",
      "holidays_events.csv   350        6        0\n",
      "   transactions.csv 83488        3        0\n",
      "\n",
      "OK - All 5 support files loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Create summary dictionary\n",
    "support_files_summary = {\n",
    "    'File': ['stores.csv', 'items.csv', 'oil.csv', 'holidays_events.csv', 'transactions.csv'],\n",
    "    'Rows': [len(df_stores), len(df_items), len(df_oil), len(df_holidays), len(df_transactions)],\n",
    "    'Columns': [df_stores.shape[1], df_items.shape[1], df_oil.shape[1], df_holidays.shape[1], df_transactions.shape[1]],\n",
    "    'Missing': [0, 0, 43, 0, 0]\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(support_files_summary)\n",
    "print(\"Support Files Summary:\")\n",
    "print(df_summary.to_string(index=False))\n",
    "print(\"\\nOK - All 5 support files loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bb81d4",
   "metadata": {},
   "source": [
    "## 3. Inspect train.csv with Dask\n",
    "\n",
    "**Objective:** Load large train.csv file using Dask and inspect structure without loading full dataset into memory\n",
    "\n",
    "**Activities:**\n",
    "- Use Dask to read train.csv (479 MB file)\n",
    "- Display schema and estimated row count\n",
    "- Check for missing values\n",
    "- Sample first rows for validation\n",
    "- Document file characteristics\n",
    "\n",
    "**Expected output:** \n",
    "- Train data structure confirmed\n",
    "- Missing value percentages calculated\n",
    "- Memory-efficient inspection complete\n",
    "\n",
    "**Note:** train.csv is too large for pandas (125M rows). Dask enables lazy evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc743af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train.csv with Dask (this may take a moment)...\n",
      "OK - train.csv loaded (Dask DataFrame)\n",
      "\n",
      "Columns: ['id', 'date', 'store_nbr', 'item_nbr', 'unit_sales', 'onpromotion']\n",
      "Data types:\n",
      "id                       int64\n",
      "date           string[pyarrow]\n",
      "store_nbr                int64\n",
      "item_nbr                 int64\n",
      "unit_sales             float64\n",
      "onpromotion            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load train.csv with Dask (lazy evaluation)\n",
    "print(\"Loading train.csv with Dask (this may take a moment)...\")\n",
    "df_train = dd.read_csv(DATA_RAW / 'train.csv')\n",
    "\n",
    "print(\"OK - train.csv loaded (Dask DataFrame)\")\n",
    "print(f\"\\nColumns: {list(df_train.columns)}\")\n",
    "print(f\"Data types:\")\n",
    "print(df_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4dee56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing row count (this will take time - processing 125M rows)...\n",
      "OK - Total rows: 125,497,040\n",
      "\n",
      "Computing missing values per column...\n",
      "\n",
      "Missing Values:\n",
      "  id                         0 (   0.0%)\n",
      "  date                       0 (   0.0%)\n",
      "  store_nbr                  0 (   0.0%)\n",
      "  item_nbr                   0 (   0.0%)\n",
      "  unit_sales                 0 (   0.0%)\n",
      "  onpromotion       21,657,651 ( 17.26%)\n"
     ]
    }
   ],
   "source": [
    "# Compute actual row count (triggers computation)\n",
    "print(\"Computing row count (this will take time - processing 125M rows)...\")\n",
    "train_length = len(df_train)\n",
    "print(f\"OK - Total rows: {train_length:,}\")\n",
    "\n",
    "# Check missing values per column\n",
    "print(\"\\nComputing missing values per column...\")\n",
    "missing_counts = df_train.isnull().sum().compute()\n",
    "missing_pct = (missing_counts / train_length * 100).round(2)\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "for col in df_train.columns:\n",
    "    count = missing_counts[col]\n",
    "    pct = missing_pct[col]\n",
    "    print(f\"  {col:<15} {count:>12,} ({pct:>6}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b61e518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling first 1000 rows...\n",
      "\n",
      "First 5 rows:\n",
      "   id        date  store_nbr  item_nbr  unit_sales  onpromotion\n",
      "0   0  2013-01-01         25    103665        7.00          NaN\n",
      "1   1  2013-01-01         25    105574        1.00          NaN\n",
      "2   2  2013-01-01         25    105575        2.00          NaN\n",
      "3   3  2013-01-01         25    108079        1.00          NaN\n",
      "4   4  2013-01-01         25    108701        1.00          NaN\n",
      "\n",
      "Basic statistics for unit_sales:\n",
      "count   1000.00\n",
      "mean       5.87\n",
      "std        8.31\n",
      "min        0.50\n",
      "25%        1.00\n",
      "50%        3.00\n",
      "75%        7.00\n",
      "max       90.00\n",
      "Name: unit_sales, dtype: float64\n",
      "\n",
      "Negative unit_sales in sample: 0 rows\n"
     ]
    }
   ],
   "source": [
    "# Sample first 1000 rows to inspect data\n",
    "print(\"Sampling first 1000 rows...\")\n",
    "df_train_sample = df_train.head(1000, npartitions=-1)\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_train_sample.head())\n",
    "\n",
    "print(\"\\nBasic statistics for unit_sales:\")\n",
    "print(df_train_sample['unit_sales'].describe())\n",
    "\n",
    "# Check for negative values\n",
    "negative_count = (df_train_sample['unit_sales'] < 0).sum()\n",
    "print(f\"\\nNegative unit_sales in sample: {negative_count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb391ec",
   "metadata": {},
   "source": [
    "## 4. Identify Guayas Stores\n",
    "\n",
    "**Objective:** Filter stores to Guayas region for project scope\n",
    "\n",
    "**Activities:**\n",
    "- Query stores.csv WHERE state = 'Guayas'\n",
    "- Count Guayas stores\n",
    "- Display store types and clusters in Guayas\n",
    "- Export Guayas store_nbr list for train filtering\n",
    "\n",
    "**Expected output:** \n",
    "- List of Guayas store identifiers\n",
    "- Guayas store characteristics (types, clusters, cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a34975b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stores in dataset: 54\n",
      "Stores in Guayas: 11\n",
      "Percentage: 20.4%\n",
      "\n",
      "Guayas stores:\n",
      "    store_nbr       city   state type  cluster\n",
      "23         24  Guayaquil  Guayas    D        1\n",
      "25         26  Guayaquil  Guayas    D       10\n",
      "26         27      Daule  Guayas    D        1\n",
      "27         28  Guayaquil  Guayas    E       10\n",
      "28         29  Guayaquil  Guayas    E       10\n",
      "29         30  Guayaquil  Guayas    C        3\n",
      "31         32  Guayaquil  Guayas    C        3\n",
      "33         34  Guayaquil  Guayas    B        6\n",
      "34         35     Playas  Guayas    C        3\n",
      "35         36   Libertad  Guayas    E       10\n",
      "50         51  Guayaquil  Guayas    A       17\n"
     ]
    }
   ],
   "source": [
    "# Filter stores to Guayas region\n",
    "guayas_stores = df_stores[df_stores['state'] == 'Guayas'].copy()\n",
    "\n",
    "print(f\"Total stores in dataset: {len(df_stores)}\")\n",
    "print(f\"Stores in Guayas: {len(guayas_stores)}\")\n",
    "print(f\"Percentage: {len(guayas_stores)/len(df_stores)*100:.1f}%\")\n",
    "\n",
    "print(\"\\nGuayas stores:\")\n",
    "print(guayas_stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12ff4310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store types in Guayas:\n",
      "type\n",
      "A    1\n",
      "B    1\n",
      "C    3\n",
      "D    3\n",
      "E    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Store clusters in Guayas:\n",
      "cluster\n",
      "1     2\n",
      "3     3\n",
      "6     1\n",
      "10    4\n",
      "17    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Cities in Guayas:\n",
      "city\n",
      "Guayaquil    8\n",
      "Daule        1\n",
      "Playas       1\n",
      "Libertad     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Guayas store_nbr list (11 stores):\n",
      "[24, 26, 27, 28, 29, 30, 32, 34, 35, 36, 51]\n"
     ]
    }
   ],
   "source": [
    "# Analyze store types in Guayas\n",
    "print(\"Store types in Guayas:\")\n",
    "print(guayas_stores['type'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nStore clusters in Guayas:\")\n",
    "print(guayas_stores['cluster'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nCities in Guayas:\")\n",
    "print(guayas_stores['city'].value_counts())\n",
    "\n",
    "# Extract store_nbr list for filtering\n",
    "guayas_store_nbrs = guayas_stores['store_nbr'].tolist()\n",
    "print(f\"\\nGuayas store_nbr list ({len(guayas_store_nbrs)} stores):\")\n",
    "print(guayas_store_nbrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45917d0d",
   "metadata": {},
   "source": [
    "## 5. Identify Top-3 Product Families\n",
    "\n",
    "**Objective:** Determine top-3 product families by item count for scope reduction\n",
    "\n",
    "**Activities:**\n",
    "- Count unique items per product family\n",
    "- Rank families by item count\n",
    "- Select top-3 families\n",
    "- Display family characteristics\n",
    "\n",
    "**Expected output:** \n",
    "- Top-3 families list with item counts\n",
    "- Percentage of total items covered\n",
    "- Family names for train filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b96334e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total product families: 33\n",
      "Total items: 4100\n",
      "\n",
      "Top-10 families by item count:\n",
      "       family  item_count\n",
      "    GROCERY I        1334\n",
      "    BEVERAGES         613\n",
      "     CLEANING         446\n",
      "      PRODUCE         306\n",
      "        DAIRY         242\n",
      "PERSONAL CARE         153\n",
      " BREAD/BAKERY         134\n",
      "    HOME CARE         108\n",
      "         DELI          91\n",
      "        MEATS          84\n",
      "\n",
      "Top-3 families selected:\n",
      "   family  item_count\n",
      "GROCERY I        1334\n",
      "BEVERAGES         613\n",
      " CLEANING         446\n",
      "\n",
      "Top-3 families cover 2,393 items (58.4% of total)\n"
     ]
    }
   ],
   "source": [
    "# Count items per family\n",
    "items_per_family = df_items['family'].value_counts().reset_index()\n",
    "items_per_family.columns = ['family', 'item_count']\n",
    "items_per_family = items_per_family.sort_values('item_count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total product families: {len(items_per_family)}\")\n",
    "print(f\"Total items: {len(df_items)}\")\n",
    "\n",
    "print(\"\\nTop-10 families by item count:\")\n",
    "print(items_per_family.head(10).to_string(index=False))\n",
    "\n",
    "# Select top-3\n",
    "top_3_families = items_per_family.head(3)\n",
    "top_3_family_names = top_3_families['family'].tolist()\n",
    "\n",
    "print(f\"\\nTop-3 families selected:\")\n",
    "print(top_3_families.to_string(index=False))\n",
    "\n",
    "print(f\"\\nTop-3 families cover {top_3_families['item_count'].sum():,} items ({top_3_families['item_count'].sum()/len(df_items)*100:.1f}% of total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8183cc0d",
   "metadata": {},
   "source": [
    "## 6. Summary & Export Findings\n",
    "\n",
    "**Objective:** Consolidate inventory findings and export for documentation\n",
    "\n",
    "**Activities:**\n",
    "- Create comprehensive inventory summary\n",
    "- Document Guayas scope (11 stores)\n",
    "- Document top-3 families scope (2,393 items)\n",
    "- Export summary to CSV for data_inventory.md update\n",
    "- Calculate expected filtering impact on train.csv\n",
    "\n",
    "**Expected output:** \n",
    "- Complete inventory summary dictionary\n",
    "- Summary CSV exported to docs/\n",
    "- Filtering estimates documented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d93d0086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inventory Summary:\n",
      "============================================================\n",
      "stores_total                   54\n",
      "items_total                    4100\n",
      "families_total                 33\n",
      "oil_records                    1218\n",
      "oil_missing                    43\n",
      "holidays_records               350\n",
      "transactions_records           83488\n",
      "train_rows                     125497040\n",
      "train_columns                  6\n",
      "guayas_stores                  11\n",
      "guayas_store_list              [24, 26, 27, 28, 29, 30, 32, 34, 35, 36, 51]\n",
      "guayas_pct_of_stores           20.4%\n",
      "top_3_families                 ['GROCERY I', 'BEVERAGES', 'CLEANING']\n",
      "top_3_items                    2393\n",
      "top_3_pct_of_items             58.4%\n",
      "onpromotion_missing_count      21657651\n",
      "onpromotion_missing_pct        17.26%\n"
     ]
    }
   ],
   "source": [
    "# Create inventory summary with the assistance of AI\n",
    "inventory_summary = {\n",
    "    # File characteristics\n",
    "    'stores_total': len(df_stores),\n",
    "    'items_total': len(df_items),\n",
    "    'families_total': df_items['family'].nunique(),\n",
    "    'oil_records': len(df_oil),\n",
    "    'oil_missing': 43,\n",
    "    'holidays_records': len(df_holidays),\n",
    "    'transactions_records': len(df_transactions),\n",
    "    'train_rows': train_length,\n",
    "    'train_columns': len(df_train.columns),\n",
    "    \n",
    "    # Guayas scope\n",
    "    'guayas_stores': len(guayas_stores),\n",
    "    'guayas_store_list': str(guayas_store_nbrs),\n",
    "    'guayas_pct_of_stores': f\"{len(guayas_stores)/len(df_stores)*100:.1f}%\",\n",
    "    \n",
    "    # Top-3 families scope\n",
    "    'top_3_families': str(top_3_family_names),\n",
    "    'top_3_items': top_3_families['item_count'].sum(),\n",
    "    'top_3_pct_of_items': f\"{top_3_families['item_count'].sum()/len(df_items)*100:.1f}%\",\n",
    "    \n",
    "    # Data quality\n",
    "    'onpromotion_missing_count': int(missing_counts['onpromotion']),\n",
    "    'onpromotion_missing_pct': f\"{missing_pct['onpromotion']:.2f}%\",\n",
    "}\n",
    "\n",
    "print(\"Inventory Summary:\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in inventory_summary.items():\n",
    "    print(f\"{key:<30} {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08125b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK - Summary exported to: C:\\Users\\adiaz\\OneDrive\\Dokumente\\PythonScripts\\MasterClass\\Demand-forecasting-in-retail\\docs\\inventory_summary.csv\n",
      "\n",
      "DATA INVENTORY SUMMARY\n",
      "Generated: 2025-11-11 04:45\n",
      "\n",
      "FILE CHARACTERISTICS:\n",
      "- stores.csv: 54 stores\n",
      "- items.csv: 4100 items across 33 families\n",
      "- oil.csv: 1218 records (43 missing values)\n",
      "- holidays_events.csv: 350 holiday records\n",
      "- transactions.csv: 83,488 transaction records\n",
      "- train.csv: 125,497,040 rows, 6 columns\n",
      "\n",
      "PROJECT SCOPE (Guayas Region):\n",
      "- Stores selected: 11 of 54 (20.4%)\n",
      "- Store IDs: [24, 26, 27, 28, 29, 30, 32, 34, 35, 36, 51]\n",
      "- Top-3 families: ['GROCERY I', 'BEVERAGES', 'CLEANING']\n",
      "- Items covered: 2,393 of 4100 (58.4%)\n",
      "\n",
      "DATA QUALITY NOTES:\n",
      "- onpromotion missing: 21,657,651 rows (17.26%)\n",
      "- Decision: Fill with False (assume no promotion)\n",
      "- train.csv size: 479 MB (requires Dask)\n",
      "- Negative unit_sales: To be investigated in full dataset\n",
      "\n",
      "NEXT STEPS (Day 2):\n",
      "1. Filter train.csv to Guayas stores only\n",
      "2. Filter to top-3 families only\n",
      "3. Random sample 300K rows for development\n",
      "4. Begin EDA on filtered dataset\n",
      "\n",
      "\n",
      "OK - Text summary also saved to: C:\\Users\\adiaz\\OneDrive\\Dokumente\\PythonScripts\\MasterClass\\Demand-forecasting-in-retail\\docs\\inventory_summary.txt\n"
     ]
    }
   ],
   "source": [
    "# Export summary to CSV for documentation\n",
    "df_summary_export = pd.DataFrame([inventory_summary])\n",
    "output_path = DOCS / 'inventory_summary.csv'\n",
    "df_summary_export.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"OK - Summary exported to: {output_path.resolve()}\")\n",
    "\n",
    "# Also create a more readable text summary\n",
    "summary_text = f\"\"\"\n",
    "DATA INVENTORY SUMMARY\n",
    "Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\n",
    "\n",
    "FILE CHARACTERISTICS:\n",
    "- stores.csv: {len(df_stores)} stores\n",
    "- items.csv: {len(df_items)} items across {df_items['family'].nunique()} families\n",
    "- oil.csv: {len(df_oil)} records ({43} missing values)\n",
    "- holidays_events.csv: {len(df_holidays)} holiday records\n",
    "- transactions.csv: {len(df_transactions):,} transaction records\n",
    "- train.csv: {train_length:,} rows, 6 columns\n",
    "\n",
    "PROJECT SCOPE (Guayas Region):\n",
    "- Stores selected: {len(guayas_stores)} of {len(df_stores)} ({len(guayas_stores)/len(df_stores)*100:.1f}%)\n",
    "- Store IDs: {guayas_store_nbrs}\n",
    "- Top-3 families: {top_3_family_names}\n",
    "- Items covered: {top_3_families['item_count'].sum():,} of {len(df_items)} ({top_3_families['item_count'].sum()/len(df_items)*100:.1f}%)\n",
    "\n",
    "DATA QUALITY NOTES:\n",
    "- onpromotion missing: {int(missing_counts['onpromotion']):,} rows ({missing_pct['onpromotion']:.2f}%)\n",
    "- Decision: Fill with False (assume no promotion)\n",
    "- train.csv size: 479 MB (requires Dask)\n",
    "- Negative unit_sales: To be investigated in full dataset\n",
    "\n",
    "NEXT STEPS (Day 2):\n",
    "1. Filter train.csv to Guayas stores only\n",
    "2. Filter to top-3 families only\n",
    "3. Random sample 300K rows for development\n",
    "4. Begin EDA on filtered dataset\n",
    "\"\"\"\n",
    "\n",
    "print(summary_text)\n",
    "\n",
    "# Save text summary\n",
    "with open(DOCS / 'inventory_summary.txt', 'w') as f:\n",
    "    f.write(summary_text)\n",
    "\n",
    "print(f\"\\nOK - Text summary also saved to: {(DOCS / 'inventory_summary.txt').resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93c66421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NOTEBOOK COMPLETE: d01_w01_SETUP_data_inventory.ipynb\n",
      "======================================================================\n",
      "\n",
      "ACCOMPLISHMENTS:\n",
      "✓ All 5 support files loaded and validated\n",
      "✓ train.csv inspected with Dask (125,497,040 rows)\n",
      "✓ Guayas region scope defined (11 stores, 20.4% of total)\n",
      "✓ Top-3 families identified (2,393 items, 58.4% of total)\n",
      "✓ Inventory summary exported to docs/\n",
      "\n",
      "FILES CREATED:\n",
      "  - C:\\Users\\adiaz\\OneDrive\\Dokumente\\PythonScripts\\MasterClass\\Demand-forecasting-in-retail\\docs\\inventory_summary.csv\n",
      "  - C:\\Users\\adiaz\\OneDrive\\Dokumente\\PythonScripts\\MasterClass\\Demand-forecasting-in-retail\\docs\\inventory_summary.txt\n",
      "\n",
      "KEY FINDINGS:\n",
      "  - Dataset: 125M rows, 6 columns, 479 MB\n",
      "  - Scope filter: 11 stores × 2,393 items = ~2.3M potential rows\n",
      "  - Missing data: onpromotion 17.26% (will fill with False)\n",
      "  - Oil data: 43 missing values (3.5%)\n",
      "\n",
      "NEXT STEPS (Day 2):\n",
      "  1. Filter train.csv to Guayas stores\n",
      "  2. Filter to top-3 families\n",
      "  3. Random sample 300K rows\n",
      "  4. Export guayas_sample_300k.csv\n",
      "\n",
      "READY FOR DAY 2 ✓\n"
     ]
    }
   ],
   "source": [
    "# Notebook completion summary\n",
    "print(\"=\" * 70)\n",
    "print(\"NOTEBOOK COMPLETE: d01_w01_SETUP_data_inventory.ipynb\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nACCOMPLISHMENTS:\")\n",
    "print(\"✓ All 5 support files loaded and validated\")\n",
    "print(\"✓ train.csv inspected with Dask (125,497,040 rows)\")\n",
    "print(\"✓ Guayas region scope defined (11 stores, 20.4% of total)\")\n",
    "print(\"✓ Top-3 families identified (2,393 items, 58.4% of total)\")\n",
    "print(\"✓ Inventory summary exported to docs/\")\n",
    "\n",
    "print(\"\\nFILES CREATED:\")\n",
    "print(f\"  - {(DOCS / 'inventory_summary.csv').resolve()}\")\n",
    "print(f\"  - {(DOCS / 'inventory_summary.txt').resolve()}\")\n",
    "\n",
    "print(\"\\nKEY FINDINGS:\")\n",
    "print(f\"  - Dataset: 125M rows, 6 columns, 479 MB\")\n",
    "print(f\"  - Scope filter: 11 stores × 2,393 items = ~2.3M potential rows\")\n",
    "print(f\"  - Missing data: onpromotion 17.26% (will fill with False)\")\n",
    "print(f\"  - Oil data: 43 missing values (3.5%)\")\n",
    "\n",
    "print(\"\\nNEXT STEPS (Day 2):\")\n",
    "print(\"  1. Filter train.csv to Guayas stores\")\n",
    "print(\"  2. Filter to top-3 families\")\n",
    "print(\"  3. Random sample 300K rows\")\n",
    "print(\"  4. Export guayas_sample_300k.csv\")\n",
    "\n",
    "print(\"\\nREADY FOR DAY 2 ✓\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
